{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('AI_exam': conda)"
  },
  "interpreter": {
   "hash": "8223b10d0755037b83a2522a3e74003eec323edabc3c7029eb46693c406569f9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 라이브러리 import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from eunjeon import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.text import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "source": [
    "## 데이터 로드"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe print 이쁘게\n",
    "pd.set_option('display.unicode.east_asian_width', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                             title  \\\n0        황성주 박사의 생식과 건강   \n1      아름다운 청년, 대니 서의 집   \n2               여왕 패션 종이인형   \n3           더 포스터 북 by 이미소   \n4                        가죽 소품   \n...                            ...   \n65543            이성적 낙관주의자   \n65544          현산어보를 찾아서 1   \n65545              전염병의 문화사   \n65546                    서울 해법   \n65547  세상을 이루는 모든 원소 118   \n\n                                                                                     summary  \\\n0      생식을 통한 식생활 개선으로 성인병을 치료하는 방법서. 육류 등의 포화지방이나 콜레...    \n1      환경 운동가 대니 서가 공개하는 환경 친화적인 집 꾸미기. 이 책은 교외에 위치한 ...       \n2      다양한 업적을 가진 세계의 여왕 패션을 직접 입혀보고, 아름답고 다양한 드레스와 왕...     \n3      직관적인 제목처럼 권마다 포스터 작품 10점으로 가득 채워진 도서이다. 그러나 일반...      \n4      가죽공예의 기본 테크닉에서부터 명함 지갑, 동전 지갑, 물림쇠 지갑, 파우치, 안경...       \n...                                                                                  ...       \n65543  <게놈>, <붉은 여왕>의 세계적 과학저술가 매트 리들리 최신작. 진화심리, 생명과...         \n65544  --한국과학문화재단 '우수과학인증도서', 「시사저널」 '올해(2003년)의 책' 선...           \n65545  인간을 비롯한 모든 생명체는 미생물에서 비롯되었다. 하지만 인간과 병원성 미생물들의...   \n65546  수도로 정해진 지 거의 630년이 되어가는 서울은 지난 60년 동안 녹지를 제외한 ...          \n65547  모든 물질의 기본인 원소를 깔끔하게 보고 직관적으로 이해할 수 있도록 엮은 비주얼 ...     \n\n               category  \n0      Home_Cook_Beauty  \n1      Home_Cook_Beauty  \n2      Home_Cook_Beauty  \n3      Home_Cook_Beauty  \n4      Home_Cook_Beauty  \n...                 ...  \n65543          Economic  \n65544          Economic  \n65545          Economic  \n65546          Economic  \n65547          Economic  \n\n[65548 rows x 3 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 65548 entries, 0 to 65547\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   title     65548 non-null  object\n 1   summary   65516 non-null  object\n 2   category  65548 non-null  object\ndtypes: object(3)\nmemory usage: 1.5+ MB\nNone\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/raw_65548_2021-06-18.csv', index_col=0).reset_index()\n",
    "print(df)\n",
    "print(df.info())"
   ]
  },
  {
   "source": [
    "## 전처리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 중복값 및 결측값 제거"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 중복값 제거\n",
    "df.drop_duplicates(subset=['title'], inplace=True)\n",
    "print(df['title'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 제거\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "source": [
    "### feature, target 분리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title'] + \" \" + df['summary']\n",
    "Y = df[['category']]"
   ]
  },
  {
   "source": [
    "### target encoding 및 encoder 저장"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array(['Art_Culture', 'Economic', 'Economic_Management',\n       'Health_Hobby_Leisure', 'Home_Cook_Beauty', 'Novel_Poem',\n       'Reference_Book', 'Religion_Mysticism'], dtype=object)]\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 1. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "onehot_Y = encoder.fit_transform(Y)\n",
    "label = encoder.categories_\n",
    "print(label)\n",
    "print(onehot_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나중에 다른 문장을 토큰화할때 동일한 classes_로 인코딩해야함 -> 학습시킨 encoder 저장\n",
    "# pickle로 저장 / 불러오기시 원본의 데이터 타입 그대로 유지\n",
    "with open('./data/nouns_category_onehot_encoder.pickle', 'wb') as f:\n",
    "    pickle.dump(encoder, f)"
   ]
  },
  {
   "source": [
    "## 형태소 분석"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0        [황성주, 박사, 생식, 건강, 생식, 식생활, 개선, 성인병, 치료, 방법, 육류...\n1        [청년, 집, 환경, 운동가, 공개, 환경, 친화, 집, 책, 교외, 위치, 부모,...  \n2        [여왕, 패션, 종이, 인형, 다양, 업적, 세계, 여왕, 패션, 드레스, 왕관, ... \n3        [포스터, 북, 미소, 직관, 제목, 권, 포스터, 작품, 점, 도서, 일반, 종이... \n4        [가죽, 소품, 가죽, 공예, 기본, 테크닉, 명함, 지갑, 동전, 지갑, 물림쇠,...\n                                           ...                                    \n58381    [파브르, 곤충기, 년, 완결, 파브르, 곤충기, 권, 자랑, 책, 곤충, 연구, ... \n58382    [자동, 제어, 공학, 빛, 교재, 시리즈, 권, 자동, 제어, 공학, 공학, 계열... \n58383    [현산어보, 한국과학문화재단, 우수, 과학, 인증, 도서, 시사저널, 올해, 년, ...\n58384    [전염병, 문화사, 인간, 생명체, 미생물, 인간, 병원, 미생물, 역사, 관계, ...\n58385    [세상, 원소, 물질, 기본, 원소, 직관, 이해, 수, 비주얼, 백, 책, 지금,...  \nLength: 58386, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 데이터 형태소 분석\n",
    "m = Mecab()\n",
    "for i in range(len(X)):\n",
    "    X[i] = m.nouns(X[i])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    stopword\n0         아\n1         휴\n2     아이구\n3     아이쿠\n4     아이고\n..       ...\n783   없다는\n784   않을까\n785     있지\n786   있으며\n787   그대로\n\n[787 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# stopword 로드\n",
    "stopwords = pd.read_csv('../datasets/stopwords.csv', index_col=0)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수로 만들어 기존 데이터에 apply해 stopword 제거\n",
    "def delete_stopwords(lst):\n",
    "    words = []\n",
    "    for word in lst:\n",
    "        if word not in list(stopwords['stopword']) and len(word) > 1:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "X = X.apply(delete_stopwords)"
   ]
  },
  {
   "source": [
    "## 토큰화 및 저장"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[17307, 319, 3289, 45, 3289, 1585, 787, 4422, 157, 5, 3018, 8331, 866, 4378, 2087, 4423, 63, 45, 22761, 458, 514, 3289, 808, 214, 78]\n"
     ]
    }
   ],
   "source": [
    "# stopword 제거한 뉴스 타이틀 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(X)\n",
    "tokened_X = token.texts_to_sequences(X)\n",
    "print(tokened_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "68561\n"
     ]
    }
   ],
   "source": [
    "# 단어의 개수 출력 / padding한 0을 포함하기 위해 +1\n",
    "wordsize = len(token.word_index) + 1  # word_index는 토큰화한 단어와 숫자 보여줌 / 0을 포함하지 않고 1부터 시작\n",
    "print(wordsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/nouns_token.pickle', 'wb') as f:\n",
    "    pickle.dump(token, f)"
   ]
  },
  {
   "source": [
    "## 문장 길이 맞춤"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "612\n"
     ]
    }
   ],
   "source": [
    "# 형태소가 가장 많은 문장의 단어 수\n",
    "max = 0\n",
    "for i in range(len(tokened_X)):\n",
    "    if max < len(tokened_X[i]):\n",
    "        max = len(tokened_X[i])\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[   0    0    0 ...  808  214   78]\n [   0    0    0 ...   86    1  323]\n [   0    0    0 ...   96 1987 2513]\n ...\n [   0    0    0 ... 1929  219 2010]\n [   0    0    0 ...   12  117  260]\n [   0    0    0 ...  979 5462  442]]\n"
     ]
    }
   ],
   "source": [
    "# 길이가 맞게 앞쪽에 0 붙여줌\n",
    "X_pad = pad_sequences(tokened_X, max)\n",
    "print(X_pad[:10])"
   ]
  },
  {
   "source": [
    "## 데이터 저장"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(52547, 612)\n(5839, 612)\n(52547, 8)\n(5839, 8)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_pad, onehot_Y, test_size = 0.1\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ASIA-23\\anaconda3\\envs\\AI_exam\\lib\\site-packages\\numpy\\core\\_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "xy = X_train, X_test, Y_train, Y_test\n",
    "np.save(f'./data/nouns_books_data_max_{max}_size_{wordsize}.npy', xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}